---
title: "About me"
layout: archive
permalink: /about me/
---

## About me

Welcome to my academic page! I am excited to share with you my journey and research endeavors in the field of artificial intelligence. Currently, I am an intern at ByteDance AI Lab, where I contribute to the cutting-edge research conducted by the Responsible AI (RAI) team. My primary focus within this team is on Reinforcement Learning with Human Feedback (RLHF), a fascinating area that merges the power of machine learning with human expertise.

In addition to my internship, I am also pursuing a Ph.D. in Computer Science starting in the fall of 2024. My research interests lie in the field of Natural Language Processing (NLP), with a particular emphasis on LLM alignment, reward modeling, and LLM agents. I am passionate about exploring how language models can be leveraged to enhance various aspects of AI and contribute to the development of more advanced and responsible AI systems.



## Research Interests
- LLM Alignment
∗ My current research interest lies in the field of LLM Alignment, specifically focusing on Reinforcement Learning
from Human Feedback (RLHF). RLHF has the potential to be applied in various domains, including tool learning,
task planning, and even debates, all of which can be abstracted as agents. Consequently, I am eager to explore
additional areas related to AI safety, such as higher-level Artificial Intelligence. My goal is to discover new
possibilities for cognitive AI, support the development of performant AI systems at scale, create user-friendly
frameworks, and facilitate the practical deployment of AI in real-world scenarios.

– Reward Modeling
∗ Reward model is a crucial module for alignment in both SFT and RLHF, showcasing its potential for the
traditional learning paradigm by providing robust and informative preferences to select optimal trajectories
or sentences from multiple models’ or human responses. Additionally, the utility of the reward model can
be enhanced through techniques like process-supervised reward modeling, which has demonstrated success in
challenging reasoning tasks such as math. However, the generalization of reward models heavily relies on curated
pairwise training data and the presence of vague scalar values can lead to issues such as reward hacking. Therefore,
I am committed to exploring a more robust and scalable reward model to address these challenges.
