---
permalink: /
title: "\"In the middle of difficulty lies opportunity.\" _--Albert Einstein_"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About me

Welcome to my academic page! I am excited to share with you my journey and research endeavors in the field of artificial intelligence. 

I completed my bachelor's degree at Huazhong University of Science and Technology (HUST) in 2020. Currently, I am a third-year graduate student at Fudan University, working in the [NLP Lab](https://nlp.fudan.edu.cn/nlpen/main.htm) under the guidance of [Xuanjing Huang](https://scholar.google.com/citations?user=AnBUn0QAAAAJ&hl=en) as my advisors, with [Qi Zhang](http://qizhang.info/) as my co-advisor.

Currently, I have the privilege of interning at ByteDance AI Lab, where I actively contribute to the cutting-edge research conducted by the Responsible AI(RAI) team. Under the guidance of my supervisors, **[Liu Yang](http://www.yliuu.com/)** and **Xiaoying Zhang**. I have the opportunity to delve into the captivating realm of Reinforcement Learning from Human Feedback (RLHF). My time at ByteDance AI Lab has been nothing short of extraordinary, providing me with invaluable experiences and opportunities for growth.

I am passionate about exploring how language models can be leveraged to enhance various aspects of AI and contribute to the development of more advanced and responsible AI systems. And I am also applying for a Ph.D. in Computer Science starting in the fall of 2024!



## Research Interests
My research interests primarily revolve around Natural Language Processing (NLP), with a specific focus on LLM alignment, including reward modeling. Additionally, I am eager to explore the realm of LLM agents and contribute to advancements in that area.

## Projects

[MOSS-RLHF](https://openlmlab.github.io/MOSS-RLHF/) (An open-source RLHF project aims to help LLMs to achieve alignment easily)


## Publications
### Mitigating Length Bias in Reinforcement Learning from Human Feedback (EMNLP 2023 Findings)

_**Wei Shen**&#9733;, Rui Zheng&#9733;, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang_

### Improving Generalization of Alignment with Human Preferences through Group Invariant Learning (Preprint)

_Rui Zheng*&#9733;, **Wei Shen**&#9733;, Yuan Hua, Wenbin Lai,  Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang_

### Delve into PPO: Implementation Matters for Stable RLHF Training (Instruction Workshop @ NeurIPS 2023)

_Rui Zheng&#9733;, Shihan Dou&#9733;, Songyang Gao&#9733;, **Wei Shen**, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao, Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang, Weng, Wensen Cheng, Yuan Hua, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang_
